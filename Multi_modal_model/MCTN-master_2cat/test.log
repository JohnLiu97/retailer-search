nohup: ignoring input
Using TensorFlow backend.
Loading data from disk...

FORMING SEQ2SEQ MODEL...

***SEQ2SEQ translates from text to audio***
Input Dims = (608, 193, 768)
Output (Translation) Dims = (608, 193, 768)
Output (Regression) Dims = (608,)

Creating TRANSLATION SEQ2SEQ model ...
Creating REGRESSION model ...
BUILDING A JOINT END-TO-END MODEL
Compiling model
Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 193, 768)     0                                            
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 193, 32)      205056      input_1[0][0]                    
                                                                 recurrent_sequential_2[0][0]     
__________________________________________________________________________________________________
lstm_layer (LSTM)               (None, 193, 32)      8320        bidirectional_1[0][0]            
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 193, 1)       33          lstm_layer[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 193)          0           time_distributed_1[0][0]         
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 193)          0           flatten_1[0][0]                  
__________________________________________________________________________________________________
repeat_vector_1 (RepeatVector)  (None, 32, 193)      0           activation_69[0][0]              
__________________________________________________________________________________________________
private__optional_input_place_h (2,)                 0                                            
__________________________________________________________________________________________________
private__optional_input_place_h (2,)                 0                                            
__________________________________________________________________________________________________
permute_1 (Permute)             (None, 193, 32)      0           repeat_vector_1[0][0]            
__________________________________________________________________________________________________
recurrent_sequential_2 (Recurre (None, 193, 768)     33857       bidirectional_1[0][0]            
                                                                 private__optional_input_place_hol
                                                                 private__optional_input_place_hol
                                                                 bidirectional_1[1][0]            
                                                                 private__optional_input_place_hol
                                                                 private__optional_input_place_hol
__________________________________________________________________________________________________
multiply_37 (Multiply)          (None, 193, 32)      0           lstm_layer[0][0]                 
                                                                 permute_1[0][0]                  
__________________________________________________________________________________________________
lambda_97 (Lambda)              (None, 32)           0           multiply_37[0][0]                
__________________________________________________________________________________________________
regression_output (Dense)       (None, 1)            33          lambda_97[0][0]                  
==================================================================================================
Total params: 247,299
Trainable params: 247,299
Non-trainable params: 0
__________________________________________________________________________________________________
None
END2END MODEL CREATED!
PREP FOR TRAINING...
TRAINING NOW...

Cannot load weight. Training from scratch

2019-11-07 19:44:00.731816: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-07 19:44:01.237068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-11-07 19:44:01.237117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-11-07 19:44:01.675337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-07 19:44:01.675387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-11-07 19:44:01.675408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-11-07 19:44:01.675590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10421 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)

Epoch 00001: val_loss improved from inf to 1.00832, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00002: val_loss improved from 1.00832 to 0.97146, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00003: val_loss improved from 0.97146 to 0.92664, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00004: val_loss improved from 0.92664 to 0.88885, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00005: val_loss improved from 0.88885 to 0.82734, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00006: val_loss improved from 0.82734 to 0.78707, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00007: val_loss improved from 0.78707 to 0.74356, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00008: val_loss improved from 0.74356 to 0.70273, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00009: val_loss improved from 0.70273 to 0.68623, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00010: val_loss improved from 0.68623 to 0.64398, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00011: val_loss improved from 0.64398 to 0.61712, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00012: val_loss did not improve from 0.61712

Epoch 00013: val_loss improved from 0.61712 to 0.56664, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00014: val_loss improved from 0.56664 to 0.51662, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00015: val_loss improved from 0.51662 to 0.44974, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00016: val_loss improved from 0.44974 to 0.39368, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00017: val_loss improved from 0.39368 to 0.31829, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00018: val_loss improved from 0.31829 to 0.31030, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00019: val_loss did not improve from 0.31030

Epoch 00020: val_loss did not improve from 0.31030

Epoch 00021: val_loss did not improve from 0.31030

Epoch 00022: val_loss did not improve from 0.31030

Epoch 00023: val_loss improved from 0.31030 to 0.30849, saving model to ./output/t_c_attention_seq2seq_bi_directional_bimodal.h5

Epoch 00024: val_loss did not improve from 0.30849

Epoch 00025: val_loss did not improve from 0.30849
PREDICTING...
Predicting the stored test input
mae: 0.3854052463388131
corr: 0.676685166183365
mult_acc: 0.81046
/home/zhengjie/programs/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
mult f_score: 0.78866
Confusion Matrix :
[[77 15]
 [13 48]]
Classification Report :
              precision    recall  f1-score   support

       False    0.85556   0.83696   0.84615        92
        True    0.76190   0.78689   0.77419        61

   micro avg    0.81699   0.81699   0.81699       153
   macro avg    0.80873   0.81192   0.81017       153
weighted avg    0.81822   0.81699   0.81746       153

Accuracy: 0.8169934640522876
